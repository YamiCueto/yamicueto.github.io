# robots.txt for yamicueto.github.io
# This file tells search engine crawlers which pages or files they can or can't request from your site.

# Allow all bots to crawl the site
User-agent: *
Allow: /

# Disallow crawling of development and build files
Disallow: /node_modules/
Disallow: /.git/
Disallow: /.github/
Disallow: /.vscode/
Disallow: /src/
Disallow: /docs/

# Sitemap location
Sitemap: https://yamicueto.github.io/sitemap.xml
